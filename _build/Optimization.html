---
redirect_from:
  - "/optimization"
interact_link: content/Optimization.ipynb
kernel_name: julia-1.3
has_widgets: false
title: |-
  Optimization
prev_page:
  url: /Differential_Equations/DifferentialEquations_Package.html
  title: |-
    DifferentialEquations_Package
next_page:
  url: /Graphs.html
  title: |-
    Graphs
suffix: .ipynb

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Optimization">Optimization<a class="anchor-link" href="#Optimization"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="k">using</span> <span class="n">PyPlot</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Overview">Overview<a class="anchor-link" href="#Overview"> </a></h2><p>The goal of optimization is to maximize (or minimize) the value of a function (called objective function) subject to a number of restrictions (called constraints). It is of fundamental importance in wide variety of fields throughout science, engineering, finance.</p>
<p>Here we will give a brief overview on the manual implementation of <em>gradient-based optimizers</em> without constraints. We will also introduce one of the many Julia optimization packages, <code>Optim</code>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gradient-based-optimization">Gradient-based optimization<a class="anchor-link" href="#Gradient-based-optimization"> </a></h2><p>While there are so-called zeroth-order methods which can optimize a function without the gradient,
most applications use first-order method which require the gradient. We will also show an example
of a second-order method, Newton's method, which require the Hessian matrix (that is, second derivatives).</p>
<p>In our examples, we will optimize the following function of the vector $x$ with two components:</p>
$$
f(x) = -\sin\left(\frac{x_1^2}{2} - \frac{x_2^2}{4} + 3 \right) \cos\left(2x_1 + 1 -e^{x_2}\right)
$$<p>Since $f$ only depends on two degrees of freedom, we can easily visualize it in the plane.
Below we define the function in Julia, and create a helper function to plot a 2D function using contours.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="o">/</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>

<span class="k">function</span> <span class="n">fplot</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">range</span><span class="p">(</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span> <span class="k">for</span> <span class="n">y</span> <span class="kp">in</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="kp">in</span> <span class="n">x</span><span class="p">]</span>
    
    <span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s">&quot;k&quot;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">contourf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
    <span class="n">axis</span><span class="p">(</span><span class="s">&quot;equal&quot;</span><span class="p">)</span>
    <span class="n">colorbar</span><span class="p">()</span>
    <span class="k">return</span>
<span class="k">end</span>

<span class="n">fplot</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/Optimization_4_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that although the function is only two-dimensional, it is fairly complex with strong nonlinearities and multiple local extrema.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Gradient-function">Gradient function<a class="anchor-link" href="#Gradient-function"> </a></h3><p>Since our methods will be gradient based, we also need to differentiate $f(x)$
to produce the gradient $\nabla f(x)$. Since this can be difficult to obtain,
or at least highly time consuming, later we will explore alternatives to this
such as numerical differentiation and automatic (symbolic) differentiation.
But to begin with it is convenient to have a function for the gradient, which we
define below.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="k">function</span> <span class="n">df</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="o">/</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span>
    <span class="n">a2</span> <span class="o">=</span> <span class="mi">2</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span><span class="o">*</span><span class="n">cos</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">sin</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span><span class="o">*</span><span class="n">sin</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">b1</span> <span class="o">-</span> <span class="mi">2</span><span class="n">b2</span><span class="p">,</span> <span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">b1</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">*</span><span class="n">b2</span><span class="p">]</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>df (generic function with 1 method)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Gradient-descent">Gradient descent<a class="anchor-link" href="#Gradient-descent"> </a></h3><p>The <em>gradient descent method</em>, or <em>steepest descent</em>, is based on the observation that for any
given value of $x$, the negative gradient $-\nabla f(x)$ gives the direction of the fastest
decrease of $f(x)$. This means that there should be a scalar $\alpha$ such that</p>
$$
f(x - \alpha\nabla f(x)) &lt; f(x)
$$<p>Assuming such an $\alpha$ can be found, the method then simply starts at some <em>initial guess</em> $x_0$
and iterates</p>
$$
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
$$<p>until some appropriate termination criterion is satisfied. With some assumptions, the method can be
shown to converge to a local minimum.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In our first implementation below, we simply set $\alpha_k$ to a specified constant.
We also use $\|\nabla f(x)\|_2$ for the termination criterion (which goes to zero at local minima).
In order to study the methods properties, we output all of the steps $x_k$ in a vector.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="k">function</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">0.1</span><span class="p">;</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">maxiter</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">df</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">gradient</span><span class="o">.^</span><span class="mi">2</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">tol</span>
            <span class="k">break</span>
        <span class="k">end</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">α</span><span class="o">*</span><span class="n">gradient</span>
        <span class="n">push!</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">xs</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>gradient_descent (generic function with 2 methods)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can now run the method on our test problem. We first define a helper function to plot the
"path" of the gradient descent method:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="k">function</span> <span class="n">plot_path</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">first</span><span class="o">.</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">last</span><span class="o">.</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="s">&quot;w.&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">first</span><span class="o">.</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">last</span><span class="o">.</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="s">&quot;r&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>plot_path (generic function with 1 method)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we define three initial guesses $x_0$, which are close to different local minima:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">x0s</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.2</span><span class="p">,</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]];</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally we run the code for each $x_0$ and plot the paths. We also output the
length of the paths, and the gradient norm at the final iteration.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">fplot</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">title</span><span class="p">(</span><span class="s">&quot;Gradient descent, fixed </span><span class="se">\$\\</span><span class="s">alpha</span><span class="se">\$</span><span class="s">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x0</span> <span class="kp">in</span> <span class="n">x0s</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plot_path</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
    <span class="n">println</span><span class="p">(</span><span class="s">&quot;Path length = </span><span class="si">$</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span><span class="s">, ||gradient|| = </span><span class="si">$</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">df</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="k">end</span><span class="p">])</span><span class="o">.^</span><span class="mi">2</span><span class="p">)))</span><span class="s">&quot;</span><span class="p">)</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/Optimization_16_0.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Path length = 501, ||gradient|| = 1.4715843307921153
Path length = 67, ||gradient|| = 8.918594790414185e-5
Path length = 501, ||gradient|| = 1.7575802156689122
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We note that the method can indeed find minima, but suffers from a few problems:</p>
<ul>
<li><p>The selection of $\alpha$ is manual and non-obvious. If it is too large, the iterations might
end up too far from the minimum. If it is too small, a large number of iterations are needed.
We will solve this below by introducing so-called <em>line searches</em>.</p>
</li>
<li><p>The method "zig-zags", in particular if $\alpha$ is too large. This is a fundamental problem
with the gradient descent method, and the reason that we will look at better search directions
(such as Newton's method).</p>
</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Line-searches">Line searches<a class="anchor-link" href="#Line-searches"> </a></h3><p>One way to improve the gradient descent method is to use a <em>line search</em> to determine
$\alpha_k$. The idea is simple: instead of using a fixed $\alpha_k$, we minimize
the one-dimensional function $f(\alpha) = f(x_k - \alpha\nabla f(x_k))$. This can be
done in many ways, but here we use a simple strategy of successively increasing $\alpha$
(by factors of $2$) as long as $f$ decreases.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="k">function</span> <span class="n">line_search</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">direction</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">αmin</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="o">^</span><span class="mi">20</span><span class="p">,</span> <span class="n">αmax</span><span class="o">=</span><span class="mi">2</span><span class="o">^</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">αmin</span>
    <span class="n">fold</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">true</span>
        <span class="k">if</span> <span class="n">α</span> <span class="o">≥</span> <span class="n">αmax</span>
            <span class="k">return</span> <span class="n">α</span>
        <span class="k">end</span>
        <span class="n">fnew</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">α</span><span class="o">*</span><span class="n">direction</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">fnew</span> <span class="o">≥</span> <span class="n">fold</span>
            <span class="k">return</span> <span class="n">α</span><span class="o">/</span><span class="mi">2</span>
        <span class="k">else</span>
            <span class="n">fold</span> <span class="o">=</span> <span class="n">fnew</span>
        <span class="k">end</span>
        <span class="n">α</span> <span class="o">*=</span> <span class="mi">2</span>
    <span class="k">end</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>line_search (generic function with 3 methods)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we write a new gradient descent method which uses line searches instead of
fixed $\alpha_k$. It is a very minor change to the previous function and we could
easily have written one general function to handle both these cases, but for
simplicity we create a new function.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="k">function</span> <span class="n">gradient_descent_line_search</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">x0</span><span class="p">;</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">maxiter</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">df</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">gradient</span><span class="o">.^</span><span class="mi">2</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">tol</span>
            <span class="k">break</span>
        <span class="k">end</span>
        <span class="n">α</span> <span class="o">=</span> <span class="n">line_search</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">α</span><span class="o">*</span><span class="n">gradient</span>
        <span class="n">push!</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">xs</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>gradient_descent_line_search (generic function with 1 method)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Running the same test case as before with this function, we see that it automatically
determines appropriate $\alpha_k$ values and converge for all three initial guesses.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">fplot</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">title</span><span class="p">(</span><span class="s">&quot;Gradient descent, line searches for </span><span class="se">\$\\</span><span class="s">alpha</span><span class="se">\$</span><span class="s">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x0</span> <span class="kp">in</span> <span class="n">x0s</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">gradient_descent_line_search</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
    <span class="n">plot_path</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
    <span class="n">println</span><span class="p">(</span><span class="s">&quot;Path length = </span><span class="si">$</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span><span class="s">, ||gradient|| = </span><span class="si">$</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">df</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="k">end</span><span class="p">])</span><span class="o">.^</span><span class="mi">2</span><span class="p">)))</span><span class="s">&quot;</span><span class="p">)</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/Optimization_23_0.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Path length = 47, ||gradient|| = 7.870693264979224e-5
Path length = 19, ||gradient|| = 2.7076595197863102e-5
Path length = 34, ||gradient|| = 6.99663461959244e-5
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Numerical-gradients">Numerical gradients<a class="anchor-link" href="#Numerical-gradients"> </a></h3><p>An alternative to implementing the gradient function <code>df</code> by hand as above, it can
be computed numerically using finite differences:</p>
$$
\left(\nabla f(x)\right)_k \approx \frac{f(x + \epsilon d^k) - f(x - \epsilon d^k)}{2\epsilon}
$$<p>Here, $\epsilon$ is a (small) step size parameter, and the vector $d^k$ is defined by $(d^k)_j = \delta_{ij}$
(that is, a zero vector with a single 1 at position $k$).</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="k">function</span> <span class="n">finite_difference_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">ϵ</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">length</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">n</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x1</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">ϵ</span>
        <span class="n">fP</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">x1</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span><span class="n">ϵ</span>
        <span class="n">fM</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">df</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fP</span> <span class="o">-</span> <span class="n">fM</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="n">ϵ</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">df</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>finite_difference_gradient (generic function with 2 methods)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can now run the previous test case, without having to compute <code>df</code>:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">fplot</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">title</span><span class="p">(</span><span class="s">&quot;Gradient descent, line searches for </span><span class="se">\$\\</span><span class="s">alpha</span><span class="se">\$</span><span class="s">, numerical gradients&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x0</span> <span class="kp">in</span> <span class="n">x0s</span>
    <span class="n">num_df</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">finite_difference_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">gradient_descent_line_search</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">num_df</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
    <span class="n">plot_path</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
    <span class="n">println</span><span class="p">(</span><span class="s">&quot;Path length = </span><span class="si">$</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span><span class="s">, ||gradient|| = </span><span class="si">$</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">df</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="k">end</span><span class="p">])</span><span class="o">.^</span><span class="mi">2</span><span class="p">)))</span><span class="s">&quot;</span><span class="p">)</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/Optimization_27_0.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Path length = 47, ||gradient|| = 7.870696304745967e-5
Path length = 19, ||gradient|| = 2.7076778424462065e-5
Path length = 34, ||gradient|| = 6.996618216493087e-5
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Newton's-method">Newton's method<a class="anchor-link" href="#Newton's-method"> </a></h3><p>Even with line searches, the gradient descent method still suffers from the zig-zag behavior
and slow convergence. If second derivatives can be obtained, <em>Newton's method</em> can converge
much faster. A simple way to describe the method is that we change the search direction in
gradient descent to $H(x_k)^{-1}\nabla f(x_k)$ instead of just $\nabla f(x_k)$, where
$H(x_k)$ is the <em>Hessian matrix</em>.</p>
<p>We compute $H(x)$ numerically using similar expressions as before (but for second derivatives):</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="k">function</span> <span class="n">finite_difference_hessian</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">ϵ</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">length</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ddf</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
    <span class="n">f0</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">n</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">ϵ</span>
        <span class="n">fP</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span><span class="n">ϵ</span>
        <span class="n">fM</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">ddf</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fP</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">f0</span> <span class="o">+</span> <span class="n">fM</span><span class="p">)</span> <span class="o">/</span> <span class="n">ϵ</span><span class="o">^</span><span class="mi">2</span>
    <span class="k">end</span>
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">n</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="o">:</span><span class="n">n</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">ϵ</span>
        <span class="n">x1</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">ϵ</span>
        <span class="n">fPP</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span><span class="n">ϵ</span>
        <span class="n">fMP</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">x1</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span><span class="n">ϵ</span>
        <span class="n">fMM</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">2</span><span class="n">ϵ</span>
        <span class="n">fPM</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">ddf</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fPP</span> <span class="o">-</span> <span class="n">fMP</span> <span class="o">-</span> <span class="n">fPM</span> <span class="o">+</span> <span class="n">fMM</span><span class="p">)</span> <span class="o">/</span> <span class="mi">4</span><span class="n">ϵ</span><span class="o">^</span><span class="mi">2</span>
        <span class="n">ddf</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ddf</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">ddf</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>finite_difference_hessian (generic function with 2 methods)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We implement Newton's method by re-using our gradient descent method with a different gradient direction.</p>
<p>Newton's method is more sensitive to the initial guess $x_0$, and in order to converge we have to move
two of our values closer to their minima. However, when the method converges it is very fast.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">fplot</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">title</span><span class="p">(</span><span class="s">&quot;Newton&#39;s method, line searches for </span><span class="se">\$\\</span><span class="s">alpha</span><span class="se">\$</span><span class="s">, numerical gradients and Hessians&quot;</span><span class="p">)</span>
<span class="n">x0s</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">0.</span><span class="p">]];</span>
<span class="k">for</span> <span class="n">x0</span> <span class="kp">in</span> <span class="n">x0s</span>
    <span class="n">search_dir</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">finite_difference_hessian</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">)</span> <span class="o">\</span> <span class="n">finite_difference_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">gradient_descent_line_search</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">search_dir</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
    <span class="n">plot_path</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
    <span class="n">println</span><span class="p">(</span><span class="s">&quot;Path length = </span><span class="si">$</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span><span class="s">, ||gradient|| = </span><span class="si">$</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">df</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="k">end</span><span class="p">])</span><span class="o">.^</span><span class="mi">2</span><span class="p">)))</span><span class="s">&quot;</span><span class="p">)</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/Optimization_31_0.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Path length = 5, ||gradient|| = 2.459525335329783e-6
Path length = 4, ||gradient|| = 1.2237414654119817e-5
Path length = 6, ||gradient|| = 1.7631665079368057e-8
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Optim-package">The <code>Optim</code> package<a class="anchor-link" href="#The-Optim-package"> </a></h2><p>For more advanced optimization method, Julia provided a number of state-of-the-art packages.
Here we will consider the <code>Optim</code> package. It has a number of options and features, but
here we will only demonstrate some basic features.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="k">using</span> <span class="n">Optim</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the simplest form, you can solve our optimization problem by only providing
the function $f(x)$ and the initial guess $x_0$. The output shows that it found
a local minimum, and that it used the "Nelder-Mead" method which is derivative
free (zeroth order).</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Results of Optimization Algorithm
 * Algorithm: Nelder-Mead
 * Starting Point: [0.0,0.5]
 * Minimizer: [0.36306816496678507,0.8476312815194897]
 * Minimum: -2.072854e-01
 * Iterations: 31
 * Convergence: true
   *  √(Σ(yᵢ-ȳ)²)/n &lt; 1.0e-08: true
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 61</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If a gradient function is available, a gradient-based solver will be used
automatically (in this case, the L-BFGS method). The <code>inplace=false</code> is used
since our gradient function <code>df</code> returns the gradient instead of modifying it.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">];</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">false</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Results of Optimization Algorithm
 * Algorithm: L-BFGS
 * Starting Point: [0.0,0.5]
 * Minimizer: [0.3631511055731168,0.8476566766300782]
 * Minimum: -2.072854e-01
 * Iterations: 8
 * Convergence: true
   * |x - x&#39;| ≤ 0.0e+00: false 
     |x - x&#39;| = 9.09e-07 
   * |f(x) - f(x&#39;)| ≤ 0.0e+00 |f(x)|: false
     |f(x) - f(x&#39;)| = -5.74e-12 |f(x)|
   * |g(x)| ≤ 1.0e-08: true 
     |g(x)| = 5.19e-10 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 20
 * Gradient Calls: 20</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The method can also be specified explicitly, e.g. the gradient descent method:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">GradientDescent</span><span class="p">();</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">false</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Results of Optimization Algorithm
 * Algorithm: Gradient Descent
 * Starting Point: [0.0,0.5]
 * Minimizer: [0.36315109114712635,0.8476566612943871]
 * Minimum: -2.072854e-01
 * Iterations: 24
 * Convergence: true
   * |x - x&#39;| ≤ 0.0e+00: false 
     |x - x&#39;| = 2.20e-08 
   * |f(x) - f(x&#39;)| ≤ 0.0e+00 |f(x)|: false
     |f(x) - f(x&#39;)| = -9.51e-15 |f(x)|
   * |g(x)| ≤ 1.0e-08: true 
     |g(x)| = 5.64e-09 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 65
 * Gradient Calls: 65</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If the gradient function is not available, it can be computed using
automatic differentiation:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">GradientDescent</span><span class="p">();</span> <span class="n">autodiff</span><span class="o">=:</span><span class="n">forward</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Results of Optimization Algorithm
 * Algorithm: Gradient Descent
 * Starting Point: [0.0,0.5]
 * Minimizer: [0.36315109114712724,0.8476566612943879]
 * Minimum: -2.072854e-01
 * Iterations: 24
 * Convergence: true
   * |x - x&#39;| ≤ 0.0e+00: false 
     |x - x&#39;| = 2.20e-08 
   * |f(x) - f(x&#39;)| ≤ 0.0e+00 |f(x)|: false
     |f(x) - f(x&#39;)| = -8.17e-15 |f(x)|
   * |g(x)| ≤ 1.0e-08: true 
     |g(x)| = 5.64e-09 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 65
 * Gradient Calls: 65</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This also works for the Hessian matrix in Newton's method. Note the extremely fast
convergence (number of iterations):</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">Newton</span><span class="p">();</span> <span class="n">autodiff</span><span class="o">=:</span><span class="n">forward</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Results of Optimization Algorithm
 * Algorithm: Newton&#39;s Method
 * Starting Point: [0.0,0.5]
 * Minimizer: [0.3631511048913647,0.84765667579412]
 * Minimum: -2.072854e-01
 * Iterations: 4
 * Convergence: true
   * |x - x&#39;| ≤ 0.0e+00: false 
     |x - x&#39;| = 4.82e-07 
   * |f(x) - f(x&#39;)| ≤ 0.0e+00 |f(x)|: false
     |f(x) - f(x&#39;)| = -8.17e-13 |f(x)|
   * |g(x)| ≤ 1.0e-08: true 
     |g(x)| = 2.90e-14 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 9
 * Gradient Calls: 9
 * Hessian Calls: 4</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we use the BFGS solver using automatic differentiation. This is a widely
used method, since it obtains convergence comparable to Newton's method but without
requiring explicit Hessian matrices:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">BFGS</span><span class="p">();</span> <span class="n">autodiff</span><span class="o">=:</span><span class="n">forward</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Results of Optimization Algorithm
 * Algorithm: BFGS
 * Starting Point: [0.0,0.5]
 * Minimizer: [0.36315109763957404,0.8476566675719984]
 * Minimum: -2.072854e-01
 * Iterations: 6
 * Convergence: true
   * |x - x&#39;| ≤ 0.0e+00: false 
     |x - x&#39;| = 1.09e-05 
   * |f(x) - f(x&#39;)| ≤ 0.0e+00 |f(x)|: false
     |f(x) - f(x&#39;)| = -1.41e-09 |f(x)|
   * |g(x)| ≤ 1.0e-08: true 
     |g(x)| = 4.04e-09 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 14
 * Gradient Calls: 14</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

 


    </main>
    